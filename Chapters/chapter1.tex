\chapter{Introduzione}
\section{Dandelion}

Dandelion è un servizio online fornito da SpazioDati che mette a disposizione dell'utente servizi di analisi semantica testuale; 
grazie ad esso è possibile, dato un testo, estrarne le entità semantiche principali (\textit{Entity Extraction}), trovare la lingua in cui è stato 
scritto (\textit{Language Detection}), classificarlo secondo modelli definiti dall'utente stesso (\textit{Text Classification}) e analizzarne la semantica 
per capire i sentimenti che l'autore ci vuole trasmettere (\textit{Sentiment Analysis}). 

Dandelion ha anche altre due RESTful-API che permettono di confrontare due testi generando un indice di similitudine fra i due (\textit{Text Similarity}) e
un motore di ricerca di entità di Wikipedia (\textit{Wikisearch}), nel caso si voglia trovare il titolo di un contenuto senza conoscerlo a priori.

L'enciclopedia alla base di Dandelion è Wikipedia, anche se a volte fra i due si colloca come mediatore dbpedia, un progetto italiano per l'estrazione di 
informazioni semi-strutturate da Wikipedia. 

Per poter usare gli end-point di Dandelion basta registrarsi sul portale dedicato e generare un token che andrà inserito come query parameter nelle chiamate https alle API;
la documentazione delle API è disponibile all'indirizzo: https://dandelion.eu/docs/.

[TODO descrizione più dettagliata di Dandelion]
%\subsection{Entity Extraction}
%\subsection{Text Similarity}
%\subsection{Text Classification}
%\subsection{Language Detection}
%\subsection{Sentiment Analysis}
%\subsection{Wikisearch}

\section{GitHub e Travis}
Per il versioning del codice si è scelto di usare il software git, appoggiandosi alla piattaforma GitHub per creare repository pubbliche. 

Le repository riguardanti il progetto di tesi sono:

\begin{itemize}
    \item Repository contenente il testo della tesi scritto in \LaTeX\ (\url{https://github.com/EdoardoLenzi9/Dandelion.LaTeX}) \\
    \item Repository contenente lo script in Python per il progetto StrepHit (\url{https://github.com/EdoardoLenzi9/Wikipedia.StrepHit}) \\
    \item Repository contenente il codice Java con le varie implementazioni dell'algoritmo per il calcolo della relatedness (la seconda parte del progetto) (\url{https://github.com/EdoardoLenzi9/Dandelion.Relatedness}) \\
    \item Repository contenente il codice sorgente del client C$\#$ (\url{https://github.com/EdoardoLenzi9/SpazioDati.Dandelion-eu})\\
\end{itemize} 

Per quanto riguarda il testing automatico è stata scelta la piattaforma Travis che permette di agganciare una repository GitHub su cui eseguire automaticamente i test 
ad ogni commit. 

Per fare ciò è necessario inserire nella repository un file di configurazione chiamato .travis.yml con i comandi necessari ad eseguire i test. 

Per esempio nel caso del client C$\#$ il file yml contiene le seguenti istruzioni:

\begin{lstlisting}[style=YmlStyle]
    language: csharp
    dist: trusty
    mono: none
    dotnet: 2.0.3

    install:
    - dotnet restore

    script:
    - dotnet build
    - dotnet test SpazioDati.Dandelion.Test/SpazioDati.Dandelion.Test.csproj
\end{lstlisting}

Mentre per lo script in python la sintassi è la seguente:
\begin{lstlisting}[style=YmlStyle]
    language: python
    python: 
      - "2.7"
    # command to install dependencies
    install:
      - pip install -r requirements.txt
    # command to run tests
    script:
      - python test.py
\end{lstlisting}

Come si nota dalle righe sopra, basta specificare il linguaggio, la versione ed i comandi (Travis esegue i test in ambiente Linux, quindi i comandi devono essere eseguibili in una bash-shell Linux) 
per installare e lanciare i test.

\section{Docker}
Per la seconda parte del progetto è stato usato Docker per creare un ambiente "chiuso", ad hoc, dove poter testare al meglio gli algoritmi senza doversi preoccupare di fattori esterni che potrebbero falsare 
il risultato del test. 

Docker permette di creare un'immagine a partire da un file di testo chiamato \lq\lq Dockerfile\rq\rq\ il quale contiene tutte le informazioni per eseguire la build dell'immagine; in pratica 
il Docker file è una sorta di descrittore dello stato di una macchina virtuale che genera una macchina virtuale persistente chiamata immagine. L'immagine può essere istanziata, divenendo un container 
il quale però non è più persistente. Una delle caratteristiche più utili di Docker è la semplicità con cui è possibile interfacciare container diversi condividendo aree di memoria sul disco e porte TCP.

Per esempio l'idea di base per monitorare le prestazioni dell'algoritmo e l'allocazione di memoria nel Docker è quella di eseguire il file jar compilato, dentro un container, con un particolare javaagent chiamato 
\href{https://github.com/prometheus/jmx_exporter}{jmx-exporter}; l'exporter si occupa di monitorare l'applicativo e di esportare in tempo reale delle metrics che vengono esposte in un server locale su una porta a scelta.
Parallelamente al primo container vengono istanziate altre due immagini, scaricate da \href{https://hub.docker.com/}{hub.docker}, chiamate prom/prometheus e grafana/grafana le quali permettono di filtrare le metrics 
generate dall'exporter e di visualizzarle direttamente su dei grafici. 

Per il setup dell'ambiente è stato creata un'immagine a partire dal seguente Dockerfile:

\begin{lstlisting}[style=YmlStyle]
    FROM openjdk:latest
    COPY . /usr/src/dandelion
    WORKDIR /usr/src/dandelion
    CMD ["java", "-Xmx6144m", "-javaagent:./lib/jmx_prometheus_javaagent-0.3.1.jar=8080:./lib/configs.yaml", "-jar", "/usr/src/dandelion/dist/Dandelion.jar"]
\end{lstlisting}

Sostanzialmente l'immagine viene edificata sopra un'altra immagine chiamata \href{https://hub.docker.com/_/openjdk/}{openjdk}, scaricata da \href{https://hub.docker.com/}{hub.docker}, contenente 
un'implementazione open source di Java SE (Java Platform, Standard Edition) a partire dalla versione 7. 
La direttiva COPY serve per copiare, al momento della build dell'immagine, tutto il contenuto della cartella contenente dentro l'ambiente virtuale alla path specificata (/usr/src/dandelion); mentre WORKDIR specifica 
la path della \textit{working directory} da cui partirà il container docker. 
Infine la direttiva CMD lancia il .jar generato dalla compilazione con il tool ant e setta come javaagent jmx-exporter (l'opzione -Xmx6144m serve solamente a specificare la massima allocazione di memoria heap, fino ad un massimo di 6GB).   

\section{Prometheus e Grafana}
[TODO]

\section{Wikidata e StrepHit}
[TODO]

\subsection{SPARQL}
[TODO]
