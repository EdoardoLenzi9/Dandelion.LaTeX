\chapter{Introduzione}
\section{Dandelion}

Dandelion è un servizio online fornito da SpazioDati che mette a disposizione dell'utente servizi di analisi semantica testuale; 
grazie ad esso è possibile, dato un testo, estrarne le entità semantiche principali (\textit{Entity Extraction}\cite{entity-extraction-demo}), trovare la lingua in cui è stato 
scritto (\textit{Language Detection}), classificarlo secondo modelli definiti dall'utente stesso (\textit{Text Classification}\cite{text-classification-demo}) e analizzarne la semantica 
per capire i sentimenti che l'autore ci vuole trasmettere (\textit{Sentiment Analysis}\cite{sentiment-analysis-demo}). 

Dandelion ha anche altre due RESTful-API\cite{rest, api} che permettono di confrontare due testi generando un indice di similitudine fra i due
(\textit{Text Similarity}\cite{text-similarity-demo}) e un motore di ricerca di entità di Wikipedia (\textit{Wikisearch}), nel caso si voglia trovare il titolo 
di un contenuto senza conoscerlo a priori.

L'enciclopedia alla base di Dandelion è Wikipedia, anche se a volte fra i due si colloca come mediatore DBpedia\footnote{
    \textbf{DBpedia}\cite{dbpedia} è un progetto che ha portato alla creazione di un OKG (Open Knowledge Graph) edificato su informazioni estratte da progetti Wikimedia\cite{wikimedia}. 
}, un progetto italiano per l'estrazione di informazioni semi-strutturate da progetti Wikimedia. 

Per poter usare gli endpoint di Dandelion basta registrarsi\cite{dandelion-registration} sul portale dedicato e generare un token che andrà inserito come query parameter nelle 
chiamate https alle API\footnote{
    La documentazione delle \textbf{API di Dandelion} è disponibile all'indirizzo: \href{https://dandelion.eu/docs/}{https://dandelion.eu/docs/}.
}.

% [\textbf{TODO} descrizione dettagliata di Dandelion]
%\subsection{Entity Extraction}
%\subsection{Text Similarity}
%\subsection{Text Classification}
%\subsection{Language Detection}
%\subsection{Sentiment Analysis}
%\subsection{Wikisearch}

\section{GitHub e Travis}
Per il versioning del codice si è scelto di usare il software Git\footnote{
    \textbf{Git}\cite{git} è un software open source, gratuito, per gestire il versioning del codice.
}, appoggiandosi alla piattaforma GitHub per creare repository pubbliche. 

Le repository riguardanti il progetto di tesi sono:

\begin{itemize}
    \item Repository contenente il testo della tesi scritto in \LaTeX\cite{latex-repo}\\
    \item Repository contenente lo script in Python per il progetto StrepHit\cite{strephit-repo}\\
    \item Repository contenente il codice Java con le varie implementazioni dell'algoritmo per il calcolo della relatedness (la seconda parte del progetto)\cite{dandelion-repo}\\
    \item Repository contenente il codice sorgente del client C$\#$\cite{client-repo} \\
\end{itemize} 

Per quanto riguarda il testing automatico è stata scelta la piattaforma Travis che permette di agganciare una repository GitHub su cui eseguire automaticamente i test 
ad ogni commit. 

Per fare ciò è necessario inserire nella repository un file di configurazione chiamato \code{.travis.yml} con i comandi necessari ad eseguire i test. 

Per esempio nel caso del client C$\#$ il file \code{.yml} contiene le seguenti istruzioni:

\begin{lstlisting}[style=YmlStyle, caption=File configurazione travis.yml per progetti C$\#$]
    language: csharp
    dist: trusty
    mono: none
    dotnet: 2.0.3

    install:
    - dotnet restore

    script:
    - dotnet build
    - dotnet test SpazioDati.Dandelion.Test/SpazioDati.Dandelion.Test.csproj
\end{lstlisting}

Mentre per lo script in Python la sintassi è la seguente:
\begin{lstlisting}[style=YmlStyle, caption=File configurazione travis.yml per progetti Python]
    language: python
    python: 
      - "2.7"
    install:
      - pip install -r requirements.txt
    script:
      - python test.py
\end{lstlisting}

Come si nota dalle righe sopra, basta specificare il linguaggio, la versione ed i comandi\footnote{
    Travis esegue i test in ambiente Linux, quindi i comandi devono essere eseguibili in una bash-shell Linux
}
per installare e lanciare i test.

\section{Docker}
Per la seconda parte del progetto è stato usato Docker\footnote{
    \textbf{Docker}\cite{docker} è un progetto open source, cross-platform, che funge da \qt{container platform provider}; 
    permette il deployment automatico di applicativi dentro ambienti virtuali chiamati container.
} per creare un ambiente "chiuso", ad hoc, dove poter testare al meglio gli algoritmi senza doversi preoccupare di fattori esterni che potrebbero falsare 
il risultato del test. 

Docker permette di creare un'immagine a partire da un file di testo, chiamato \code{Dockerfile}, che contiene tutte le informazioni per eseguire la build dell'immagine; 
in pratica il \code{Dockerfile} è una sorta di descrittore dello stato di una macchina virtuale che genera, una volta lanciata la build, una macchina virtuale persistente chiamata immagine. 

Un container è un istanza di un'immagine che diventa quindi non persistente (qualsiasi modifica allo stato della VM verrà perso una volta chiuso il container). 

Una delle caratteristiche più utili di Docker è la semplicità con cui è possibile interfacciare container diversi condividendo aree di memoria sul disco e porte TCP.

L'idea di base per monitorare le prestazioni dell'algoritmo è stata quella di compilare il progetto Java con il tool Ant\cite{ant} ed esportare il file \code{Dandelion.jar}; 
è seguita la creazione di un \code{Dockerfile} basato sull'immagine \code{openjdk}\cite{open-jdk}, 
scaricata da hub.docker\cite{hub-docker}, contenente un'implementazione open source di Java SE (Java Platform, Standard Edition) a partire dalla versione 7. 

\begin{lstlisting}[style=YmlStyle, caption=Dockerfile]
    FROM openjdk:latest
    COPY . /usr/src/dandelion
    WORKDIR /usr/src/dandelion
    CMD ["java", "-javaagent:./lib/jmx_prometheus_javaagent-0.3.1.jar=8080:./lib/configs.yaml", "-jar", "/usr/src/dandelion/dist/Dandelion.jar"]
\end{lstlisting}

La prima direttiva \code{FROM} specifica l'immagine di base su cui verrà edificata una nuova immagine, le successive due direttive \code{COPY} e \code{WORKDIR} servono rispettivamente a copiare 
tutto il contenuto della cartella corrente dentro l'ambiente virtuale alla path specificata (\textit{/usr/src/dandelion}) e a settare quest'ultima come \textit{working directory} da cui 
partirà il container docker. 

Infine la direttiva \code{CMD} specifica il comando che verrà eseguito una volta finito il setup dell'ambiente, in questo caso verrà lanciato \code{Dandelion.jar} con un particolare
\code{javaagent} chiamato JMX\footnote{
    \textbf{JMX Exporter}\cite{jmx} è un progetto GitHub, open source, che fornisce un javaagent capace di esportare ed esporre metrics per il software 
    Prometheus\cite{prometheus}.
};
questo tool è un exporter che si occupa di monitorare l'applicativo e di esportare in tempo reale le relative metrics che vengono esposte in un server locale su una porta a scelta 
(8080 in questo caso).

Per evitare di effettuare una build del \code{Dockerfile} ogni volta che modifichiamo il codice sorgente Java, è possibile mappare la cartella corrente con la working directory 
dell'immagine tramite l'opzione \code{-v} che crea un volume condiviso; tramite l'opzione \code{-p} invece è possibile mappare una porta tcp interna al docker su una esterna.

\begin{lstlisting}[style=YmlStyle, caption=Run Docker]
    docker run -it -p 8080:8080 -v <current-dir>.:<docker-working-dir> \
    --name dandelion-volume dandelion-container
\end{lstlisting}

\section{Prometheus e Grafana}
Parallelamente al primo container vengono istanziate altre due immagini, scaricate da hub.docker\cite{hub-docker}, chiamate \code{prom/prometheus}\cite{docker-prometheus} 
e \code{grafana/grafana}\cite{docker-grafana} le quali permettono di filtrare le metrics generate dall'exporter e di visualizzarle direttamente su dei grafici. 

\subsection{Prometheus}
Prometheus\cite{prometheus} è un applicativo open source rilasciato sotto Apache 2 License su GitHub; permette di salvare in memoria e/o sul disco le metrics raccolte 
da un exporter (JMX in questo caso) e di filtrarle a piacimento tramite un apposito linguaggio di query. 

È anche possibile rappresentare i risultati delle query direttamente su dei grafici statici tuttavia per questo aspetto si è scelto di usare Grafana.

\subsection{Grafana}
Grafana\cite{grafana} è anch'essa un software open source volto alla creazione di dashboard dinamiche, tipicamente popolate da grafici e vari widget di monitoraggio, 
funzionalmente molto completi e gradevoli sotto il profilo estetico. 

Grafana è nativamente predisposta per integrarsi con Prometheus, basta indicare la porta tcp su cui è settato Prometheus e la query che si desidera visualizzare.   

\section{Wikidata e StrepHit}
Wikidata è un progetto gratuito e open source legato a Wikipedia che mette a disposizione dell'utente un knowledge base accessibile sia dal portale wikidata.org che tramite un endpoint SPARQL.

I dati presenti nel knowledge base sono dati strutturati secondo le logiche del semantic web e adottano, quindi, il framework RDF (Resource Description Framework) proposto da W3C; 
pertanto è facilmente integrabile con qualsiasi altro knowledge base di terze parti che segua gli stessi schemi e modelli RDF.

Questo servizio rende possibile l'interazione con il knowledge base direttamente esponendo un endpoint che accetta query SPARQL, veicolate da chiamate http. 

\begin{lstlisting}[style=YmlStyle, caption=SPARQL endpoint]
    https://query.wikidata.org/bigdata/namespace/wdq/sparql?query={SPARQL}
\end{lstlisting}

\subsection{StrepHit}
StrepHit\cite{strephit} è un progetto nato l'anno scorso, seguito da un team in FBK, coordinato da Marco Fossati; il progetto prevedeva la creazione di un applicativo, scritto prevalentemente 
in Python, capace di analizzare testi e tradurli in Wikidata statements (o QuickStatements\cite{quickstatements}).  

QuickStatements è un tool, scritto da Magnus Manske, che definisce un linguaggio volto a descrivere/modificare elementi Wikidata. Questo linguaggio è ormai diventato uno standard 
per quanto riguarda Wikidata, tant'è che la nomenclatura degli elementi del knowledge base seguono gli standard dettati da Quickstatements.

\subsection{SPARQL}
SPARQL è il linguaggio di query definito da W3C per RDF. Le query sono molto affini a quelle scritte in SQL, infatti ne condividono molte key-word, la differenza sta nel fatto che i dati ora 
seguono i modelli RDF pertanto avranno sempre un \code{item}, una \code{property} e un \code{value}.

Wikidata mette a disposizione un'interfaccia\cite{sparql-ui} che permette di lanciare manualmente query SPARQL sul knowledge base
e un endpoint\cite{sparql-endpoint} per contattare lo stesso servizio con chiamate http. 
