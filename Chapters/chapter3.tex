\chapter{Calcolo della Relatedness}

La seconda parte del progetto riguarda l'analisi di due metodi critici (\code{readDump()} e \code{rel(int a, int b)}) che servono a 
calcolare, dati gli identificativi di due entità semantiche, il loro valore di correlazione (relatedness). 

Per correlazione fra due entità si intende, in questo contesto, un indice numerico che assume valori nell'intervallo [0,255], 
riscalato, per comodità, in una percentuale codificata in variabili di tipo \code{float}. 

Si ha a disposizione un dump dove sono salvati tutti i valori di correlazione (sopra una certa soglia minima) per ogni coppia di entità. 

Il calcolo della relatedness viene effettuato a monte,
dai gestori del DataBase che rilasciano il dump; la relatedness è fondamentale per l'algoritmo di Dandelion, basti solo pensare alla potenzialità 
di poter verificare la correlazione fra le varie parole estratte da una frase, controllando quindi anche il contesto oltre alla singola parola.

Il metodo \code{readDump()} serve a caricare in memoria i valori del dump 
sottoforma di \lq\lq matrice\rq\rq\ mentre il metodo \code{rel(int a, int b)} serve a fare una ricerca nella struttura dati per poi ritornare il valore di 
correlazione fra le entità con identificativi \code{a} e \code{b}.

Questa scelta implementativa, attualmente adottata, è sicuramente molto efficiente in termini di prestazioni ma anche molto onerosa in termini di memoria; 
pertanto il fine dell'analisi sarebbe quello di studiare/testare implementazioni alternative per ottimizzare le prestazioni e/o diminuire lo spreco di memoria.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementazione Iniziale}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Il Dump}
\begin{lstlisting}[style=TeXStyle, caption=Estratto del dump]
53676192
4922289
0.01
2
1.0
53676158 7 null
53676164 5 NnAwQT/ZvTVJ05MeSfcdI0rMrg1LBmUH
53676016 13 RDsFRUkyASRLFf3T
53675811 16 null
...
\end{lstlisting}

Analizzando la parte iniziale di un dump si nota che sulle prime cinque righe ci sono valori di inizializzazione mentre dalla sesta riga in poi troviamo i valori della matrice.

Il primo numero intero, denominato \code{max$\_$id}, indica il limite massimo che gli id delle entità possono assumere; 
dato che gli id delle entità non sono necessariamente sequenziali 
(fra due di essi potrei avere dei \lq\lq buchi\rq\rq, degli intervalli in cui gli identificativi non sono associati ad alcuna entità) 
esiste una funzione \code{map} che mappa (\lq\lq compatta\rq\rq) gli id delle entità su altri id univoci e sequenziali. 

Il secondo intero, denominato \code{nodesSize} è il valore massimo che la funzione \code{map} può assumere (limita il codominio della funzione).

\begin{equation}\begin{split}
    map():\ \mathbb{N} & \rightarrow \mathbb{N}\\
            [0,max\_id] & \rightarrow [0, nodeSize],\ max\_id \geq nodeSize\\
\end{split}\end{equation}

Continuando a leggere il dump troviamo la relatedness minima considerata \code{minRel} (sotto la quale i valori di correlazione non vengono salvati nel dump) 
e altri valori di configurazione quali \code{minIntersection} e \code{threshold} che tuttavia non ci interessano particolarmente.

La funzione \code{map} in un certo senso mappa \lq\lq al contrario\rq\rq\ gli id; infatti l'id massimo verrà mappato su 0, il penultimo id verrà mappato su 1 e così via.

Nel caso del dump in questione abbiamo:

\begin{lstlisting}[style=TeXStyle, caption=Esempio di funzione map e postingList]
MaxId = 53676192
NodeSize = 4922289

   ID		        MAP(ID)    POSTINGLIST
...
53676158    ->      7           null
53676159    ->      6           FW/I...
53676164    ->      5           NnAw...
53676174    ->      4           null
53676176    ->      3           null
53676177    ->      2           null
53676180    ->      1           null
53676190    ->      0           null
\end{lstlisting}

Dalla sesta riga del dump in poi inizia la definizione della matrice, ogni riga è composta da tre valori separati da uno spazio. 

Consideriamo una riga qualsiasi:
\begin{lstlisting}[style=TeXStyle, caption=Riga del dump]
53676164 5 NnAwQT/ZvTVJ05MeSfcdI0rMrg1LBmUH
\end{lstlisting}

Il primo valore \code{$a = 53676164$} è l'id di un entità, il secondo valore \code{$b = 5 = map(a)$} è il risultato della funzione map, 
infine il terzo valore \code{$c = NnAwQT/ZvTVJ05MeSfcdI0rMrg1LBmUH$} è una stringa di dimensione variabile oppure \code{null};
quest'ultima stringa, denominata \code{postingList}, è la codifica in \code{Base64} di un array di byte che rappresenta un dizionario chiave valore. 

\begin{center}
    \includegraphics[scale=0.55]{Sources/Img/c02_01.png}
    \captionof{figure}{Encoding del dizionario nella postingList}
\end{center}

In pratica una tupla \code{$<chiave,\ valore>$} è codificata sull'array da una sequenza di quattro byte; quindi partizionando l'array in gruppi da 4 byte 
ottengo, per ogni gruppo, la chiave (è un id \lq\lq mappato\rq\rq ) codificata sui primi tre byte ed il valore (la relatedness) definito sul quarto byte (il byte più a destra).

Questa codifica, pertanto, permette di gestire set di entità con id \lq\lq mappato\rq\rq\ massimo $2^{3 \cdot 8} \simeq 1,7 \cdot 10^7$ ed una scala di valori 
di correlazione appartenenti all'intervallo $[0,\ 255]$.

Bisogna precisare che l'array di byte, denominato \code{postingList}, è ordinato per valore crescente delle chiavi e, dato che \code{rel(a, b) = rel(b, a)},
si è deciso di salvare una sola volta il valore di correlazione imponendo la relazione: $a > b$. 

Le righe del dump, invece, non sono necessariamente ordiante per identificativo dell'entità.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Esempio}
Per calcolare \code{$relatedness(a, d)$} bisogna per prima cosa identificare la riga del dump contenente la tupla \code{$(a,\ b,\ c)$}, 
assumendo che valga la relazione \code{$map(a) > map(d)$}. 

Con una ricerca binaria sulla \code{postingList c} bisogna trovare un gruppo di quattro byte in cui i primi tre byte corrispondano a \code{map(d)}
ed il quarto byte sarà il valore di correlazione voluto. 

Ovviamente è possibile che \code{c} sia \code{null} (l'entità con id \code{a} non ha nessuna correlazione con altre entità i cui id \lq\lq mappati\rq\rq\ siano inferiori a \code{b}) oppure 
che nella \code{postingList} non sia presente nessuna chiave corrispondente a \code{map(d)}, in questi casi \code{$relatedness(a,d) = 0$}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Il Codice Nativo}

\begin{lstlisting}[style=JavaStyle, caption=Implentazione nativa]
// STRUTTURA DATI
public class RelatednessMatrix {
    public int[] map;
    public byte[][] matrix;
    public float configuredMinRel;
    public int configuredMinIntersection;
    public float threshold;
    public static int EL_SIZE = 4;
}

// CODICE UTILIZZO
public float rel(int a, int b){
    if(a==b) return 1f;

    int nodeA = data.map[a];
    int nodeB = data.map[b];
    if (nodeA < 0 || nodeB < 0) return 0f;

    int key = a>b ? nodeB : nodeA;
    byte[] array = a>b ? data.matrix[nodeA] : data.matrix[nodeB];

    if (array == null) return 0f;

    int start = 0;
    int end = array.length - RelatednessMatrix.EL_SIZE;
    int pos = -1;
    while(pos == -1 && start <= end)
    {
        int idx = ((start+end)/RelatednessMatrix.EL_SIZE)/2;
        int idx_value = ((array[idx*RelatednessMatrix.EL_SIZE] & 0xFF) << 16)
                    + ((array[idx*RelatednessMatrix.EL_SIZE+1] & 0xFF) << 8)
                    + ( array[idx*RelatednessMatrix.EL_SIZE+2] & 0xFF);

        if(idx_value == key) pos = idx;
        else{
            if(key > idx_value)
                start = (idx + 1) * RelatednessMatrix.EL_SIZE;
            else
                end = (idx - 1) * RelatednessMatrix.EL_SIZE;
        }
    }

    if(pos == -1) return 0f;
    else
    {
        byte by = array[pos * RelatednessMatrix.EL_SIZE + 3];
        int byint = by + 128;
        float byteRel = byint / 255f;
        return data.configuredMinRel + byteRel * (1 - data.configuredMinRel);
    }
}

// CODICE LETTURA
public String END_OF_FILE = "" + '\0';

public RelatednessMatrix readDump() throws Exception {
    URL url = getClass().getResource("dump.txt");
    File file = new File(url.getPath());
    BufferedReader fbr = new BufferedReader(new FileReader(file));
    String line = new String();

    try {   
        int max_id = Integer.parseInt(fbr.readLine().trim()); // Trims all leading and trailing whitespace from this string         
        int nodesSize = Integer.parseInt(fbr.readLine().trim());
        float minRel = Float.parseFloat(fbr.readLine().trim());
        int minIntersection = Integer.parseInt(fbr.readLine().trim());
        float threshold = Float.parseFloat(fbr.readLine().trim());         

        RelatednessMatrix data = new RelatednessMatrix();
        data.map = new int[max_id + 1];
        data.matrix = new byte[nodesSize][];
        data.configuredMinRel = minRel;
        data.configuredMinIntersection = minIntersection;
        data.threshold = threshold;

        int idx = 0;

        while ((line = fbr.readLine()) != null) {
            if (line.trim().equals(END_OF_FILE.trim())) {
                break;
            }
            System.out.println(line);
            String[] splittedLine = line.split("\\s+"); // split(" ")
            if (splittedLine.length != 3) {
                throw new Exception("Wrong format relatedness file for the line: " + line);
            }

            int wid = Integer.parseInt(splittedLine[0]);
            int node = Integer.parseInt(splittedLine[1]);
            data.map[wid] = node;

            if (splittedLine[2].equals("null")) {
                data.matrix[node] = null;
            } else {
                byte[] a = Base64.getDecoder().decode(splittedLine[2].toString()); 
                data.matrix[node] = Base64.getDecoder().decode(splittedLine[2].toString());
            }
            idx++;
        }
        
        if (idx != nodesSize) {
            throw new Exception("Wrong format relatedness file the number of line do not match with size of matrix");
        }
        
        return data;
    } catch (Exception e) {
        System.out.println(e.getMessage());
        return null;
    }
}
\end{lstlisting}

Il metodo \code{readDump()} legge riga per riga il dump e restituisce un'istanza di \code{RelatednessMatrix} valorizzata. 
La funzione map viene salvata in un array di interi (\code{data.map}) mentre la matrice con le correlazioni viene salvata in un 
array bidimensionale di byte (\code{data.matrix}).

Il metodo \code{rel(int a, int b)} calcola la funzione \code{map} sugli id \code{a} e \code{b}, dopodichè fa una binary search sulla \code{postingList} 
ottenuta da \code{matrix[map(a)]} (assumendo $map(a) > map(b)$). 

Se la ricerca binaria ha successo il valore di relatedness ottenuto va riscalato su una scala di valori $[0, 255]$, 
convertito in float e riscalato nuovamente in base al valore di relatedness minima considerata (per escludere i valori al di sotto della relatedness minima,
che non verranno mai usati).  

\begin{lstlisting}[style=JavaStyle, caption=Conversione della relatedness da byte a float]
byte by = array[pos * RelatednessMatrix.EL_SIZE + 3]; //considero il quarto byte 
int byint = by + 128;   //scalo di 128 valori per ottenere una relatedness in [0, 255]

//cast in float, escludendo i valori sotto la relatedness minima (configuredMinRel)
float byteRel = byint / 255f; 
return data.configuredMinRel + byteRel * (1 - data.configuredMinRel);
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Osservazioni}
Considerato che il metodo \code{readDump()} viene chiamato una sola volta per allocare in memoria la struttura dati nel server, 
non ci sono particolari vincoli temporali per la sua esecuzione; per quanto riguarda il calcolo della relatedness invece 
l'algoritmo dev'essere il più performante possibile.

Il codice nativo per il calcolo della relatedness costa $O(lg_2(n))$ (con $n=nodeSize$) per la ricerca binaria e O(1) per quanto riguarda le 
istruzioni precedenti e successive a quest'ultima.

Tolta una piccola modifica al codice che portebbe ad evitare tre moltiplicazioni per ciclo nella binary search (non è necessario dividere e poi moltiplicare per 
\code{RelatednessMatrix.EL$\_$SIZE}), non sembra si possano migliorare di molto le prestazioni dell'algoritmo senza cambiare struttura dati.

Una via percorribile per non \lq\lq pagare\rq\rq\ $O(lg_2(n))$ per la binary search potrebbe essere quella di allocare direttamente in memoria la matrice 
completa (in questo caso avrei un algoritmo di tempo costante O(1)). Questa soluzione però andrebbe a peggiorare drasticamente lo spreco di memoria.
Infatti nell'implementazione attuale la matrice ha dimensione massima $nodeSize \otimes nodeSize$ ma le righe hanno dimensione variabile e 
non occupano quindi sempre $(nodeSize -1) \cdot 4\ Byte$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Stima della Memoria Allocata}
Se consideriamo le prime righe del dump in questione possiamo stimare che mediamente il $57\%$ degli id hanno postingList vuota (null) ed il restante $43\%$ ha un numero 
medio di relatedness per id pari allo $0,0101\%$ di nodeSize.

L'implementazione attuale alloca in memoria per l'array di interi map circa:
\begin{center}
\begin{equation}\begin{split} 
    nodeSize \cdot 4\ Byte & = 4^2 \cdot 10^6 Byte = 16 MB \\
\end{split}\end{equation}
(arrotondando e senza tenere conto di fattori secondari come i 32 byte occupati da overhead e puntatore).
\end{center}
Per la matrice invece possiamo assumere che, su una macchina a 64 bit, avrò il $57\%$ di righe a null ($0.57 \cdot 4\cdot 10^6 \cdot 8\ Byte = 18\ MB$) ed il 
restante $43\%$ di righe con mediamente 404 valori di relatedness codificati su 4 Byte ($0.43 \cdot 4 \cdot 10^6 \cdot 404 \cdot 4\ Byte = 2.8\ GB$). 

Quindi in totale potremmo stimare che solo questo dump occupa quasi 3 GB (notare che esiste un dump per ogni lingua supportata da Dandelion e che questo è uno 
dei dump più piccoli).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Stima della Matrice Completa}
Se allocassimo in memoria una matrice di Byte completa, di dimensioni $nodeSize \otimes nodeSize$, essa occuperebbe $(4 \cdot 10^6)^2 = 1.6 \cdot 10^{13}\ Byte = 16\ TB$.

Pur considerando che esistono strutture dati particolarmente ottimizzate per gestire array di grandi dimensioni con pochi valori al loro interno (teniamo presente che
più della metà dei valori nella matrice non sarebbero valorizzati), come SparseArray e SuperArrayList, bisogna tener presente che spesso, tali strutture dati, 
si basano su liste; in questo caso però difficilmente si otterrebbero prestazioni migliori di quelle dell'implementazione attuale dato che il tempo di lookup 
sarebbe sempre $O(lg_2(noseSize))$. 

In conclusione, a meno di uno spreco di memoria ulteriore, difficilmente si otterrebbero buoni risultati mantenendo come struttura dati di riferimento la matrice. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Stima del problema inverso}
Una via praticabile potrebbe essere quella di vedere il problema al contrario, considerando che esistono solo 256 valori possibili di relatedness, 
per minimizzare lo spreco di memoria si potrebbe pensare all'allocazione di 256 array in cui si storicizzano tutte le coppie di id che hanno la stessa relatedness. 
In questo modo eliminerei la ridondanza costituita da valori uguali di relatedness salvati nel dump migliaia di volte.

Ci si accorge subito che anche questa via non è praticabile dato che per risparmiare 1 Byte (il valore della relatedness) dovrei allocarne altri 8 per le coppie di id (di tipo intero). 
Andrei ad occupare $0.43 \cdot 4 \cdot 10^6 \cdot 404 \cdot 2 \cdot 4 = 5.6 GB$, il doppio rispetto all'implementazione nativa, senza contare che 
le performance peggiorerebbero.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Stima implementazione di un grafo}
Si potrebbe pensare di implementare un grafo orientato, salvando per ogni nodo l'id e la lista di archi uscenti e su ogni arco la relatedness fra nodo di partenza 
e nodo di arrivo. Dovrei quindi usare una struttura dati del tipo:
 
\begin{lstlisting}[style=JavaStyle, caption=Struttura di un possibile grafo]
    public class Node{
        public int Id;
        public List<Arrow> Arrows;  
    }

    public class Arrow{
        public Node Node;
        public byte Relatedness;
    }
\end{lstlisting}

Tuttavia in questo modo sprecherei memoria perchè il puntatore al nodo successivo (ipotizzando di lavorare su una macchina a 64 bit) peserebbe più dell'identificativo 
del nodo stesso. Al che potrei sostituire il puntatore con un intero ma anche in questo caso avrei il dizionario $< Id,\ Relatedness >$ che pesa 5 Byte mentre nella 
postingList pesa 4 Byte perchè chiave e valore sono accorpati. Infine se seguissi la stessa politica di codifica del dizionario della postingList 
di fatto sarei tornato all'implementazione iniziale senza trarre alcun giovamento, se non lo svantaggio agguintivo di non poter accedere in O(1) ad un nodo arbitrario. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementazione con Hash Map}
Sembra che l'unica via praticabile per mantenere tutti i dati in memoria, diminuendo la memoria occupata e massimizzando le prestazioni sia una funzione di Hash Map, 
che mappi gli id concatenati sul valore della relatedness; se la funzione fosse ben ottimizzata otterrei in O(1) la relatedness (guadagno prestazionale)
e potenzialmente potrebbe occupare meno memoria della matrice iniziale.

Purtroppo è molto difficile stimare a priori lo spazio occupato da un Hash Map, 
il modo più rapido è confrontare con dei benchmark l'implementazione nativa contro quella basata su Hash Map.

\subsection{Codice}
\begin{lstlisting}[style=JavaStyle, caption=Implentazione con HashMap]
public float Relatedness(int a, int b) throws Exception {
    if (a == b) {
        return 1f;
    }

    int nodeA = data.map[a];
    int nodeB = data.map[b];

    if (nodeA < 0 || nodeB < 0) {
        return 0f;
    }

    String key = (nodeA > nodeB) ? (nodeA + "." + nodeB) : (nodeB + "." + nodeA);
    try {
        byte value = (byte) data.hm.get(key);

        if (value == 0) {
            return 0f;
        } else {
            int byint = value + 128;
            float byteRel = byint / 255f;
            return data.configuredMinRel + byteRel * (1 - data.configuredMinRel);
        }
    } catch (Exception e) {
        return 0f;
    }
}

public RelatednessHashMap ReadDump(String dump) throws Exception {
    URL url = getClass().getResource(dump);
    File file = new File(url.getPath());
    BufferedReader fbr = new BufferedReader(new FileReader(file));
    String line = new String();

    try {
        RelatednessHashMap data = new RelatednessHashMap();
        int max_id = Integer.parseInt(fbr.readLine().trim());
        int nodesSize = Integer.parseInt(fbr.readLine().trim());
        data.configuredMinRel = Float.parseFloat(fbr.readLine().trim());
        data.configuredMinIntersection = Integer.parseInt(fbr.readLine().trim());
        data.threshold = Float.parseFloat(fbr.readLine().trim());

        data.map = new int[max_id + 1];
        data.hm = new HashMap();

        String END_OF_FILE = "" + '\0';

        while ((line = fbr.readLine()) != null) {
            if (line.trim().equals(END_OF_FILE.trim())) {
                break;
            }

            String[] splittedLine = line.split("\\s+");
            if (splittedLine.length != 3) {
                throw new Exception("Wrong format relatedness file for the line: " + line);
            }

            int wid = Integer.parseInt(splittedLine[0]);
            int node = Integer.parseInt(splittedLine[1]);
            data.map[wid] = node;

            if (!splittedLine[2].equals("null")) {
                byte[] postingList = Base64.getDecoder().decode(splittedLine[2].toString());

                int idx_value = ((postingList[0] & 0xFF) << 16)
                            + ((postingList[1] & 0xFF) << 8)
                            + ((postingList[2] & 0xFF) << 0);

                String key = idx_value + "." + node;
                data.hm.put(key, postingList[3]);
            }
        }

        return data;
    } catch (Exception e) {
        System.out.println(e.getMessage());
        return null;
    }
}
\end{lstlisting}

\subsection{Test}

\subsubsection{Generazione di un dump con dimensione arbitraria}
Per testare al meglio le varie implementazioni è stato creato un metodo per generare automaticamente un dump con dati random, della dimensione che si desidera;
il metodo \code{CreateDump()} accetta come parametri \code{maxId}, \code{nodeSize} e \code{nullPercentage} (la percentuale di righe con \code{postingList = null}) e genera: un dump 
analogo a quello reale, un dump con \code{postingList} non codificata in Base64 (utile per debug) e un file contenente una serie di test-cases della forma: 
\code{$<entityId; entityId; relatedness>$}.

Per essere più fedeli possibile all'originale le \code{postingList == null} sono uniformemente distribuite fra le righe (non tutte ammassate all'inizio o alla fine) e la lunghezza delle restanti 
è direttamente proporzionale al valore dell'id della riga (questo per poter avere casi di test su \code{postingList} di lunghezza massima e minima). 
Con questo metodo è anche molto facile stimare il numero di relazioni presenti nel dataset, basta calcolare l'area del triangolo che ha altezza e base pari a \code{nodeSize} (togliendo 
le \code{postingList null}).

\begin{center}
    \includegraphics[scale=0.45]{Sources/Img/c03_01.png}
    \captionof{figure}{Dump con struttura a triangolo}
\end{center}

\subsubsection{Test Hash Map}
Per testare le nuove implementazioni sono stati eseguiti gli stessi test-cases sullo stesso dump e messi a confronto tempi di risposta e occupazione di memoria della struttura dati.\\

\begin{tabular}{|l|l|l|l|l|}
    \hline
    \textbf{Type}    &   \textbf{MaxId - NodeSize}   &   \textbf{$\#$ test-cases}   &   \textbf{Read dump time}  &   \textbf{Testing time}            \\      
    \hline
    Native              &   1.000 - 900              &   1.000.000                  &   0.09s               &   0.90s                       \\
    HashMap             &   1.000 - 900              &   1.000.000                  &   0.17s               &   1.05s                       \\         
    \hline
    Native              &   10.000 - 9.000           &   1.000.000                  &   1.73s               &   1.16s                       \\
    HashMap             &   10.000 - 9.000           &   1.000.000                  &   20.24s              &   2.00s                       \\ 
    \hline
    Native              &   15.000 - 14.000          &   1.000.000                  &   4.91s               &   1.26s                       \\
    HashMap             &   15.000 - 14.000          &   1.000.000                  &   72.26s              &   2.42s                       \\ 
    \hline
    Native              &   20.000 - 19.000          &   1.000.000                  &   8.87s               &   1.32s                       \\
    HashMap             &   20.000 - 19.000          &   1.000.000                  &   133.32s             &   3.29s                       \\ 
    \hline
    Native              &   30.000 - 29.000          &   1.000.000                  &   19.71s              &   1.49s                       \\
    HashMap             &   30.000 - 29.000          &   1.000.000                  &   out of memory       &   out of memory               \\
    \hline                                    
\end{tabular}\\

Sono stati eseguiti vari test, a macchina scarica, con entrambe le implementazioni su dump di dimensione sempre crescente. 
Ogni test è stato replicato 10 volte e, con le medie dei risultati, è stata popolata la tabella sopra riportata.

L'implementazione nativa sembra superare sotto ogni aspetto l'implementazione con HashMap.

Come si può notare dalla tabella l'implementazione con HashMap impiega molto più tempo della nativa per leggere il dump e caricare in memoria la struttura dati;
questo non sembrerebbe un grosso problema perchè il tempo di lettura non influisce sulle performance dell'algoritmo potrebbe essere un problema se la funzione di crescita 
fosse esponenziale rispetto al numero di relatedness presenti nel dump. 

Anche se la funzione cresce molto velocemente, rappresentando i dati di test su un grafico, sembra sia comunque lineare (per il dump reale si stima che impieghi meno di un ora).

\begin{center}
    \includegraphics[scale=0.55]{Sources/Img/c03_02.jpg}
    \captionof{figure}{Funzione di crescita del tempo di lettura del dump al crescere del numero correlazioni fra entità (in rosso implementazione con HashMap, in blu implementazione nativa)}
\end{center}

Il grosso problema dell'implementazione con HashMap è lo spreco di memoria, l'ultimo test, con \code{nodeSize = 30.000}, è fallito perchè alla lettura del dump il container docker aveva allocato il 
78$\%$ della memoria della macchina (con 16GB di ram) contro il 13$\%$ utilizzato dall'implementazione nativa. 

Altra cosa inaspettata, al crescere del numero di correlazioni l'HashMap diventa così complessa da risolvere che, pur essendo stimato il tempo di look-up a $O(1)$, comunque supera il tempo 
di ricerca binaria ($O(lg_2(n)$)) con uno scostamento sempre crescente.\\

Stima del numero di correlazioni nel dump originale: 
\begin{equation}\begin{split}
    49.22289 \cdot (4922289 \cdot 0,000101) \cdot 0.43 = 1.041.843.947\\
\end{split}\end{equation}

Stima del numero di correlazioni con \code{nodeSize = 29.000}; 
\begin{equation}\begin{split}
    29.000 \cdot 29.000 \cdot 0.5 \cdot 0.43 = 180.815.000 \\
\end{split}\end{equation}

Nonostante ci sia un ordine di grandezza di differenza fra il numero di relatedness storicizzate nel dump originale e quello nel dump autogenerato (con \code{maxId = 30000} e \code{nodeSize = 29.000}), 
l'implementazione con HashMap risulta estremamente inefficiente rispetto a quella nativa. 